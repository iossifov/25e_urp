{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-qiL6OTJLOK"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from scipy.io import loadmat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yC4yLh6GJoGr"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "import logging\n",
        "logging.getLogger('matplotlib.font_manager').disabled = True\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H9ETi50BJPbl"
      },
      "outputs": [],
      "source": [
        "# @title Plotting Functions\n",
        "\n",
        "def plot_glm_matrices(X, y, nt=50):\n",
        "  \"\"\"Show X and Y as heatmaps.\n",
        "\n",
        "  Args:\n",
        "    X (2D array): Design matrix.\n",
        "    y (1D or 2D array): Target vector.\n",
        "\n",
        "  \"\"\"\n",
        "  from matplotlib.colors import BoundaryNorm\n",
        "  from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "  Y = np.c_[y]  # Ensure Y is 2D and skinny\n",
        "\n",
        "  f, (ax_x, ax_y) = plt.subplots(\n",
        "    ncols=2,\n",
        "    figsize=(6, 8),\n",
        "    sharey=True,\n",
        "    gridspec_kw=dict(width_ratios=(5, 1)),\n",
        "  )\n",
        "  norm = BoundaryNorm([-1, -.2, .2, 1], 256)\n",
        "  imx = ax_x.pcolormesh(X[:nt], cmap=\"coolwarm\", norm=norm)\n",
        "\n",
        "  ax_x.set(\n",
        "    title=\"X\\n(lagged stimulus)\",\n",
        "    xlabel=\"Time lag (time bins)\",\n",
        "    xticks=[4, 14, 24],\n",
        "    xticklabels=['-20', '-10', '0'],\n",
        "    ylabel=\"Time point (time bins)\",\n",
        "  )\n",
        "  plt.setp(ax_x.spines.values(), visible=True)\n",
        "\n",
        "  divx = make_axes_locatable(ax_x)\n",
        "  caxx = divx.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "  cbarx = f.colorbar(imx, cax=caxx)\n",
        "  cbarx.set_ticks([-.6, 0, .6])\n",
        "  cbarx.set_ticklabels(np.sort(np.unique(X)))\n",
        "\n",
        "  norm = BoundaryNorm(np.arange(y.max() + 1), 256)\n",
        "  imy = ax_y.pcolormesh(Y[:nt], cmap=\"Blues\", norm=norm)\n",
        "  ax_y.set(\n",
        "    title=\"y\\n(spike count)\",\n",
        "    xticks=[]\n",
        "  )\n",
        "  ax_y.invert_yaxis()\n",
        "  plt.setp(ax_y.spines.values(), visible=True)\n",
        "\n",
        "  divy = make_axes_locatable(ax_y)\n",
        "  caxy = divy.append_axes(\"right\", size=\"30%\", pad=0.1)\n",
        "  cbary = f.colorbar(imy, cax=caxy)\n",
        "  cbary.set_ticks(np.arange(y.max()) + .5)\n",
        "  cbary.set_ticklabels(np.arange(y.max()))\n",
        "\n",
        "# we will call this for comparison of two filters\n",
        "def plot_spike_filter(theta, dt, show=True, **kws):\n",
        "  \"\"\"Plot estimated weights based on time lag model.\n",
        "\n",
        "  Args:\n",
        "    theta (1D array): Filter weights, not including DC term.\n",
        "    dt (number): Duration of each time bin.\n",
        "    kws: Pass additional keyword arguments to plot()\n",
        "    show (boolean): To plt.show or not the plot.\n",
        "  \"\"\"\n",
        "  d = len(theta)\n",
        "  t = np.arange(-d + 1, 1) * dt\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.plot(t, theta, marker=\"o\", **kws)\n",
        "  ax.axhline(0, color=\".2\", linestyle=\"--\", zorder=1)\n",
        "  ax.set(\n",
        "    xlabel=\"Time before spike (s)\",\n",
        "    ylabel=\"Filter weight\",\n",
        "  )\n",
        "  if show:\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JHZreYiYJXdN"
      },
      "outputs": [],
      "source": [
        "# @title Data retrieval and loading\n",
        "import os\n",
        "import hashlib\n",
        "import requests\n",
        "\n",
        "fname = \"RGCdata.mat\"\n",
        "url = \"https://osf.io/mzujs/download\"\n",
        "expected_md5 = \"1b2977453020bce5319f2608c94d38d0\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
        "      print(\"!!! Data download appears corrupted !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBxi-_5IIpgX"
      },
      "source": [
        "***Acknowledgement***:\n",
        "\n",
        "This notebook is modified from a [introduction to GLM tutoral](https://github.com/NeuromatchAcademy/course-content) created by Neuromatch Academy.\n",
        "\n",
        "---\n",
        "\n",
        "# **P2**  GLMs for Encoding\n",
        "\n",
        "#### The objective is to model a retinal ganglion cell spike train by fitting a temporal receptive field. First with a Linear-Gaussian GLM (also known as ordinary least-squares regression model) and then with a Poisson GLM (aka \"Linear-Nonlinear-Poisson\" model).\n",
        "\n",
        "This tutorial is designed to run with retinal ganglion cell spike train data from [Uzzell & Chichilnisky 2004](https://journals.physiology.org/doi/full/10.1152/jn.01171.2003?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed).\n",
        "\n",
        "\n",
        "we use data from an experiment that presented a screen which randomly alternated between two luminance values and recorded responses from retinal ganglion cell (RGC), a type of neuron in the retina in the back of the eye. This kind of visual stimulus is called a \"full-field flicker\", and it was presented at ~120Hz (ie. the stimulus presented on the screen was refreshed about every 8ms). These same time bins were used to count the number of spikes emitted by each neuron.\n",
        "\n",
        "The file `RGCdata.mat` contains three variables:\n",
        "\n",
        "- `Stim`, the stimulus intensity at each time point. It is an array with shape $T \\times 1$, where $T=144051$.\n",
        "\n",
        "-  `SpCounts`, the binned spike counts for 2 ON cells, and 2 OFF cells. It is a $144051 \\times 4$ array, and each column has counts for a different cell.\n",
        "\n",
        "- `dtStim`, the size of a single time bin (in seconds), which is needed for computing model output in units of spikes / s. The stimulus frame rate is given by `1 / dtStim`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZj267-rIzsg"
      },
      "outputs": [],
      "source": [
        "# Read the data from a Matlab file\n",
        "\n",
        "data = loadmat('RGCdata.mat')  # loadmat is a function in scipy.io\n",
        "dt_stim = data['dtStim'].item()  # .item extracts a scalar value\n",
        "\n",
        "# Extract the stimulus intensity\n",
        "stim = data['Stim'].squeeze()  # .squeeze removes dimensions with 1 element\n",
        "\n",
        "# Extract the spike counts for one cell\n",
        "cellnum = 2\n",
        "spikes = data['SpCounts'][:, cellnum]\n",
        "\n",
        "# Choose not to use all of the timepoints in the dataset, for speed\n",
        "keep_timepoints = 20000\n",
        "stim = stim[:keep_timepoints]\n",
        "spikes = spikes[:keep_timepoints]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhmXBDXLLLG3"
      },
      "source": [
        "First, let us visualize the changes in stimulus intensities and spike counts over time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "XTmuCOjYLGIk",
        "outputId": "3efe5fcf-278a-40af-a52f-c9b8c02908eb"
      },
      "outputs": [],
      "source": [
        "# Total number of points to plot\n",
        "nt = 120\n",
        "timepoints = np.arange(nt)\n",
        "time = timepoints * dt_stim\n",
        "\n",
        "f, (ax_stim, ax_spikes) = plt.subplots(nrows=2, sharex=True, figsize=(8, 5))\n",
        "ax_stim.plot(time, stim[timepoints])\n",
        "ax_stim.set_ylabel('Stimulus intensity')\n",
        "\n",
        "# Set x-axis at y=0 for ax_stim\n",
        "ax_stim.spines['bottom'].set_position(('data', 0))\n",
        "ax_stim.spines['top'].set_visible(False)\n",
        "ax_stim.spines['right'].set_visible(False)\n",
        "ax_stim.xaxis.set_ticks_position('bottom')\n",
        "ax_stim.set_axisbelow(False)\n",
        "\n",
        "ax_spikes.plot(time, spikes[timepoints])\n",
        "ax_spikes.set_xlabel('Time (s)')\n",
        "ax_spikes.set_ylabel('Number of spikes')\n",
        "\n",
        "f.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViy2FjGMms1"
      },
      "source": [
        "### Create design matrix\n",
        "\n",
        "Our goal is to predict the cell's activity from the stimulus intensities preceding it. That will help us understand how RGCs process information over time. To do so, we first need to create the *design matrix* for this model, which organizes the stimulus intensities in matrix form such that the $i$th row has the stimulus frames preceding timepoint $i$.\n",
        "\n",
        "We will create the design matrix $\\mathbf{X}$ using $d=25$ time lags. That is, $\\mathbf{X}$ should be a $T \\times d$ matrix. $d = 25$ (about 200 ms) is a choice we're making based on our prior knowledge of the temporal window that influences RGC responses. In practice, you might not know the right duration to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "KCzS0t5BMl7P",
        "outputId": "1332562b-4697-4a8f-93e6-57e6212cf3a6"
      },
      "outputs": [],
      "source": [
        "d = 25\n",
        "# Construct a matrix where each row has the d frames of\n",
        "# the stimulus preceding and including timepoint t\n",
        "T = len(stim)\n",
        "# Total number of timepoints (hint: number of stimulus frames)\n",
        "X = np.zeros((T, d))\n",
        "# Add placeholder zeros to the earlest sliding windows\n",
        "padded_stim = np.concatenate([np.zeros(d - 1), stim])\n",
        "\n",
        "for t in range(T):\n",
        "  X[t] = padded_stim[t:t+25]\n",
        "\n",
        "# Visualize\n",
        "plot_glm_matrices(X, spikes, nt=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhhzFfEDNIj7"
      },
      "source": [
        "### Fit Linear-Gaussian regression model\n",
        "\n",
        "\n",
        "First, we will use the design matrix to compute the maximum likelihood estimate for a linear-Gaussian GLM (aka \"general linear model\"), simply assuming the firing rate (0 or 1) is a linear sum of past stimuli weighed by the filter $\\theta$, for which we will infer the value for.\n",
        "\n",
        "There are two approaches to obtain $\\theta$, and interestingly for the simple linear filter, both approaches achieve the same analytical solution.\n",
        "\n",
        "#### **Approach 1**: Mean Square Error (MSE)\n",
        "\n",
        "We can think about the temporal receptive field as a function, and the best parameters are obtained when the average error between the function prediction $\\hat y$ and the true $y$ is the smallest.\n",
        "\n",
        "\\begin{equation}\n",
        "\\boldsymbol{\\theta}^*=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmin}}\\|X \\boldsymbol{\\theta}-\\boldsymbol{y}\\|_2^2=\\sum_{t=1}^T\\left(y_t-\\boldsymbol{\\theta}^{\\top} \\boldsymbol{x}_t\\right)^2\n",
        "\\end{equation}\n",
        "\n",
        "Since this objective function is convex, the minimum is achieved when the gradient is zero:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial\\|X \\boldsymbol{\\theta}-\\boldsymbol{y}\\|_2^2}{\\partial \\boldsymbol{\\theta}} & =2 X^{\\top}(X \\boldsymbol{\\theta}-\\boldsymbol{y})=0 \\\\\n",
        "\\Rightarrow \\boldsymbol{\\theta}_{\\mathrm{MSE}} & =\\left(X^{\\top} X\\right)^{-1} X^{\\top} \\boldsymbol{y}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "#### **Approach 2**: Maximum Likelihood Estimator (MLE)\n",
        "We can think about the temporal receptive field as a generative model, then we would want a model with parameters that maximize the likelihood of seeing the observed data (the firing rate). Given gaussian noise,  \n",
        "\n",
        "\\begin{equation}\n",
        "\\boldsymbol{\\theta}^*=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\log L (\\boldsymbol{\\theta} \\mid X, \\boldsymbol{y})=-\\frac{Td}{2} \\log 2 \\pi \\sigma^2-\\frac{1}{2 \\sigma^2}(\\boldsymbol{y}-X \\boldsymbol{\\theta})^{\\top}(\\boldsymbol{y}-X \\boldsymbol{\\theta})\n",
        "\\end{equation}\n",
        "\n",
        "The objective is quadratic in $\\theta$, meaning that we can obtain the maximum when gradient is zero:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "& \\frac{\\partial \\log L (\\boldsymbol{\\theta} \\mid X, \\boldsymbol{y})}{\\partial \\boldsymbol{\\theta}}=-\\frac{1}{\\sigma^2} X^{\\top}(X \\boldsymbol{\\theta}-\\boldsymbol{y})=0 \\\\\n",
        "& \\Rightarrow \\boldsymbol{\\theta}_{\\mathrm{MLE}}=\\left(X^{\\top} X\\right)^{-1} X^{\\top} \\boldsymbol{y}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "We obtain the same close-form solution!\n",
        "\n",
        "\n",
        "Note: Before we can apply this equation, we would need to augment the design matrix to account for the mean of $y$, because the spike counts are all $\\geq 0$. We do this by adding a constant column of 1's to the design matrix, which will allow the model to learn an additive offset weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOyr-5ptLJL8"
      },
      "outputs": [],
      "source": [
        "# Build the full design matrix with the constant column\n",
        "y = spikes\n",
        "constant = np.ones_like(y)\n",
        "X = np.column_stack([constant,X])\n",
        "\n",
        "# Get the MLE weights for the LG model\n",
        "theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "theta_lg = theta[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ6oHWVXNnZd"
      },
      "source": [
        "Plot the resulting maximum likelihood filter estimate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "7xvl9alANiGZ",
        "outputId": "e12ea8cd-b271-464d-c2ac-be39edd9a377"
      },
      "outputs": [],
      "source": [
        "# plot temporal filter we have inferred:\n",
        "\n",
        "d = len(theta_lg) # length filtered weights\n",
        "t = np.arange(-d + 1, 1) * dt_stim # actual time in seconds\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.plot(t, theta_lg, marker=\"o\")\n",
        "ax.axhline(0, color=\".2\", linestyle=\"--\", zorder=1)\n",
        "ax.set(\n",
        "  xlabel=\"Time before spike (s)\",\n",
        "  ylabel=\"Filter weight\",\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSsg9joZN2Ks"
      },
      "source": [
        "Now we can also visualize how well our temporal filter model describe this neuron's firing patter by using it to predicting the spiking. The way to do it is just to compute predicted  $\\mathbf{\\hat y} = X \\boldsymbol{\\hat \\theta}$, and plot it agaist $\\mathbf{y}$ firing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "CAiMAYuEN02_",
        "outputId": "6f251f63-a672-469d-d010-42f672b90170"
      },
      "outputs": [],
      "source": [
        "yhat = X @ theta\n",
        "\n",
        "# Visualize\n",
        "nt = 50\n",
        "t0 = 120\n",
        "\n",
        "t = np.arange(t0, t0 + nt) * dt_stim\n",
        "\n",
        "f, ax = plt.subplots()\n",
        "lines = ax.stem(t, spikes[:nt], use_line_collection=True)\n",
        "plt.setp(lines, color=\".5\")\n",
        "lines[-1].set_zorder(1)\n",
        "ax.plot(t, yhat[:nt], linewidth = 3)\n",
        "ax.set(\n",
        "    xlabel=\"Time (s)\",\n",
        "    ylabel=\"Spikes\",\n",
        ")\n",
        "ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
        "ax.legend([lines[0], yhat], [\"Spikes\", \"Predicted\"])\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XZn3dYOOq7c"
      },
      "source": [
        "* Is this a good model?\n",
        "\n",
        "The prediction line more-or-less follows the bumps in the spikes, but it never predicts as many spikes as are actually observed. And, more troublingly, it's predicting *negative* spikes for some time points.\n",
        "\n",
        "The Poisson GLM will help to address these failures.\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "### **Bonus:**Linear-Nonlinear-Poisson GLM\n",
        "\n",
        "\n",
        "In this exercise, we will use [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) to compute maximum likelihood estimates for the filter weights in the Poisson GLM model with an exponential nonlinearity (LNP: Linear-Nonlinear-Poisson).\n",
        "\n",
        "In practice, this will involve filling out two functions.\n",
        "\n",
        "- The first should be an *objective function* that takes a design matrix, a spike count vector, and a vector of parameters. It should return a negative log likelihood.\n",
        "- The second function should take `stim` and `spikes`, build the design matrix and then use `minimize` internally, and return the MLE parameters.\n",
        "\n",
        "What should the objective function look like? We want it to return the negative log likelihood: $-\\log P(y \\mid \\mathbf{X}, \\theta).$\n",
        "\n",
        "In the Poisson GLM,\n",
        "\n",
        "\\begin{equation}\n",
        "\\log P(\\mathbf{y} \\mid \\mathbf{X}, \\theta) = \\sum_t \\log P(y_t \\mid \\mathbf{x_t},\\theta),\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\n",
        "\\begin{equation}\n",
        "P(y_t \\mid \\mathbf{x_t}, \\theta) = \\frac{\\lambda_t^{y_t}\\exp(-\\lambda_t)}{y_t!} \\text{, with rate } \\lambda_t = \\exp(\\mathbf{x_t}^{\\top} \\theta).\n",
        "\\end{equation}\n",
        "\n",
        "Now, taking the log likelihood for all the data we obtain:\n",
        "\n",
        "\\begin{equation}\n",
        "\\log P(\\mathbf{y} \\mid X, \\theta) = \\sum_t( y_t \\log\\left(\\lambda_t) - \\lambda_t - \\log(y_t !)\\right).\n",
        "\\end{equation}\n",
        "\n",
        "Because we are going to minimize the negative log likelihood with respect to the parameters $\\theta$, we can ignore the last term that does not depend on $\\theta$. For faster implementation, let us rewrite this in matrix notation:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{y}^{\\top} \\log(\\mathbf{\\lambda}) - \\mathbf{1}^{\\top} \\mathbf{\\lambda} \\text{, with  rate } \\mathbf{\\lambda} = \\exp(\\mathbf{X} \\theta)\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWF1l-hIbPjf"
      },
      "outputs": [],
      "source": [
        "def neg_log_lik_lnp(theta, X, y):\n",
        "  \"\"\"Return -loglike for the Poisson GLM model.\n",
        "\n",
        "  Args:\n",
        "    theta (1D array): Parameter vector.\n",
        "    X (2D array): Full design matrix.\n",
        "    y (1D array): Data values.\n",
        "\n",
        "  Returns:\n",
        "    number: Negative log likelihood.\n",
        "\n",
        "  \"\"\"\n",
        "  # Compute the Poisson log likelihood\n",
        "  rate = np.exp(X @ theta)\n",
        "  log_lik = y @ np.log(rate) - rate.sum()\n",
        "  return -log_lik\n",
        "\n",
        "\n",
        "def fit_lnp(stim, spikes, d=25):\n",
        "  \"\"\"Obtain MLE parameters for the Poisson GLM.\n",
        "\n",
        "  Args:\n",
        "    stim (1D array): Stimulus values at each timepoint\n",
        "    spikes (1D array): Spike counts measured at each timepoint\n",
        "    d (number): Number of time lags to use.\n",
        "\n",
        "  Returns:\n",
        "    1D array: MLE parameters\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Build the design matrix\n",
        "  y = spikes\n",
        "  constant = np.ones_like(y)\n",
        "\n",
        "  # Use a random vector of weights to start (mean 0, sd .2)\n",
        "  x0 = np.random.normal(0, .2, d + 1)\n",
        "\n",
        "  # Find parameters that minmize the negative log likelihood function\n",
        "  res = minimize(neg_log_lik_lnp, x0, args=(X, y))\n",
        "\n",
        "  return res[\"x\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "tHjIxniHNqDo",
        "outputId": "d872ad6e-0d9e-44a7-9560-b8bfa9d8c5ac"
      },
      "outputs": [],
      "source": [
        "# Fit LNP model\n",
        "theta_lnp = fit_lnp(stim, spikes)\n",
        "\n",
        "plot_spike_filter(theta_lg[1:], dt_stim, show=False, color=\".5\", label=\"LG\")\n",
        "plot_spike_filter(theta_lnp[1:], dt_stim, show=False, label=\"LNP\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7F5xstWQGEU"
      },
      "source": [
        "Plotting the LG and LNP weights together, we see that they are broadly similar, but the LNP weights are generally larger. What does that mean for the model's ability to *predict* spikes? It will turn out that LNP model does a better job of fitting the actual spiking data. Importantly, it never predicts negative spikes!\n",
        "\n",
        "If you are interested, try to plot the predicted $\\mathbf {\\hat y} $ using the filter inferred from the LNP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCzriTNrQb_E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
